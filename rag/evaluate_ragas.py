'''Evaluates RAG model answers using an LLM.
Processes interaction logs and writes structured evaluations to a JSON file.'''
import json
import time
from pathlib import Path
import sys
from llm_connector import LLMConnector


project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


CREDENTIALS_PATH = project_root / 'assets' / 'secrets' / 'credentials.json'
LOGS_PATH = project_root / 'rag' / 'interaction_logs.jsonl'
OUTPUT_FILE_PATH = project_root / 'rag' / 'evaluation_results.json'

MAX_RETRIES = 3
RETRY_DELAY_SECONDS = 5

llm = LLMConnector(CREDENTIALS_PATH)

JUDGE_PROMPT_TEMPLATE = """
You are an impartial AI judge evaluating the quality of an answer generated by a Retrieval Augmented Generation model.
Your task is to assess the answer based on the given question and the provided context.

Please evaluate the answer based on the following criteria:
1.  **Faithfulness (Groundedness):**
    *   Is the answer entirely supported by the provided context?
    *   Does the answer introduce any information not found in the context (hallucination)?
    *   Score 1-5 (1: Not at all faithful, 5: Perfectly faithful).
2.  **Relevance (Helpfulness):**
    *   Does the answer directly and comprehensively address the user's question?
    *   Is the answer on-topic?
    *   Score 1-5 (1: Not at all relevant, 5: Perfectly relevant).
3.  **Clarity and Conciseness:**
    *   Is the answer clear, easy to understand, and to the point?
    *   Does it avoid unnecessary jargon or verbosity?
    *   Score 1-5 (1: Very unclear/verbose, 5: Perfectly clear and concise).

**Input:**
Question: {question}
Context: {contexts}
Answer: {answer}

**Evaluation Task:**
Provide your evaluation as a JSON object with the following keys:
- "faithfulness_score" (integer 1-5)
- "faithfulness_reasoning" (string explanation)
- "relevance_score" (integer 1-5)
- "relevance_reasoning" (string explanation)
- "clarity_score" (integer 1-5)
- "clarity_reasoning" (string explanation)
- "overall_assessment" (string, a brief summary of the answer's quality)
- "suggested_improvement" (string, if any, how the answer could be improved, otherwise "None")

Ensure your output is ONLY the JSON object, without any introductory text or ```json ... ``` markers.

JSON Evaluation:
"""

def get_llm_evaluation(question: str, contexts: str, answer: str) -> dict:
    '''
    Sends the data to the LLM for evaluation and attempts to parse its JSON response.
    '''

    prompt = JUDGE_PROMPT_TEMPLATE.format(
        question=question,
        contexts=contexts,
        answer=answer
    )

    for attempt in range(MAX_RETRIES):
        try:
            response_content = llm.generate_answer(prompt)

            if response_content.strip().startswith('```json'):
                response_content = response_content.strip()[7:-3].strip()
            elif response_content.strip().startswith('```'):
                response_content = response_content.strip()[3:-3].strip()


            evaluation_data = json.loads(response_content)
            return evaluation_data
        except Exception as e:
            print(f'An unexpected error occurred with LLM (Attempt {attempt+1}/{MAX_RETRIES}): {e}')
            if attempt == MAX_RETRIES - 1:
                return {'error': f'LLM API call failed after retries: {str(e)}'}
            time.sleep(RETRY_DELAY_SECONDS * (2**attempt))

    return {'error': 'LLM evaluation failed after all retries'}


def main():
    '''Reads interaction logs,
      evaluates each entry using an LLM, and writes results to an output file.'''
    evaluated_count = 0
    try:
        with open(LOGS_PATH, 'r', encoding='utf-8') as infile, \
             open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as outfile:

            for i, line in enumerate(infile):
                try:
                    data = json.loads(line.strip())
                except json.JSONDecodeError:
                    print(f'Skipping malformed JSON line {i+1}: {line.strip()}')
                    error_entry = {'original_line_number': i+1, 'error': 'Malformed JSON',
                                    'raw_line': line.strip()}
                    outfile.write(json.dumps(error_entry) + '\n')
                    continue

                question = data.get('question')
                answer = data.get('answer')
                contexts = data.get('contexts')
                # timestamp = data.get('timestamp')

                if not all([question, answer, contexts]):
                    print('Skipping line' +
                          f'{i+1} due to missing \'question\', \'answer\', or \'contexts\'')
                    error_entry = {'original_line_number': i+1,
                                    'error': 'Missing critical fields', 'data': data}
                    outfile.write(json.dumps(error_entry) + '\n')
                    continue

                print(f'\nProcessing entry {i+1}...')
                print(f'  Question: {question[:100]}...')

                if isinstance(contexts, list):
                    contexts_str = '\n\n'.join([str(c) for c in contexts])
                else:
                    contexts_str = str(contexts)

                evaluation = get_llm_evaluation(question, contexts_str, answer)

                output_entry = {
                    'original_data': data,
                    'llm_evaluation': evaluation
                }
                outfile.write(json.dumps(output_entry) + '\n')
                outfile.flush()
                evaluated_count += 1
                print('Evaluation for entry' +
                      f"{i+1} completed. Result: {evaluation.get('overall_assessment', 'N/A')}")

    except FileNotFoundError:
        print(f'Error: Input file \'{LOGS_PATH}\' not found')
        return
    except Exception as e:
        print(f'An unexpected error occurred during file processing: {e}')
        return

    print('\n--- Evaluation Complete ---')

if __name__ == "__main__":
    main()
